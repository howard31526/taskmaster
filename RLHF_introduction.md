# 🧩 一、RLHF（Reinforcement Learning from Human Feedback）

RLHF 的目標是讓模型更符合人類的偏好，不只是單純生成語法正確的文字，而是回答得「更像人」。

整個流程有三個階段：

## (1) 監督微調（SFT, Supervised Fine-Tuning）

- 用人工整理的「指令 → 高品質回答」資料訓練模型。
  
- 這個階段只是讓模型「會講話」。

## (2) 獎勵模型（Reward Model, RM）

- 準備資料：同一個問題給模型兩個回答，請人類標哪個比較好。
  
- 用這些「偏好對」訓練一個 RM，使它能根據回答好壞給分數。
  
- RM 只負責打分，不參與生成。

## (3) 強化學習（Policy Optimization）

- 模型（policy）開始生成回答。

- RM 給這些回答打分作為「獎勵」。

- 模型根據獎勵學習如何生成更高分的回答。

- 但為了防止模型改太多導致語言品質崩壞（例如亂講話、胡編），我們會加上 KL 散度懲罰項 —— 讓模型不要偏離「參考模型（通常是凍結的 SFT 模型）」太遠。

🔹這個懲罰的概念是：

「你可以更有創意，但別離你原本的語氣太遠。」

### 也就是：

- RM：告訴模型「哪種答案好」

- KL 懲罰：告訴模型「別亂跑太遠」

兩者平衡後，模型既學會人類偏好，又保留原本流暢的語言能力。

## 💡 實務細節：

- RLHF 的強化學習階段通常用 PPO（Proximal Policy Optimization） 來實作。

- 參考模型（reference model）會被凍結，作為 KL 懲罰的基準，不再更新。

- 常見的超參數：

  - KL 系數 β ≈ 0.01～0.2

  - 用於訓練 RM 的偏好資料量：10,000 對以上效果會比較穩定。

 ---

# ⚙️ 二、PPO（Proximal Policy Optimization）

PPO 是 RLHF 裡「讓模型穩定更新」的技術核心。

傳統強化學習會有一個問題：模型更新太快、太大，容易導致性能崩壞。
PPO 解決的方法是：

**每次只允許模型做「小幅調整」，像是「安全更新模式」。**

換句話說，模型每次生成新回答後，會被比較：

- 新的生成結果（policy）

- 舊的參考結果（reference / old policy）

如果差距太大，PPO 會自動削弱更新幅度。這樣訓練過程不會不穩定，也能避免語言能力突然下降。

### PPO 是一個 *技巧性很重* 的步驟，訓練代價也高，因為要不斷生成、打分、再更新模型，是個回圈式過程。

---

# 🔁 三、DPO（Direct Preference Optimization）

DPO 是 RLHF 的「簡化版」。

目標一樣：讓模型更符合人類偏好，但它完全不需要 Reward Model，也不用強化學習。

## 🔹原理：

DPO 直接用偏好對 (好回答, 壞回答) 來訓練，
讓模型對「好回答」的信心（log probability）更高、
對「壞回答」的信心更低。

它透過一個固定的「參考模型」（通常是 SFT 凍結模型）
作為比較基準，確保模型不會偏離原來的語言風格。

這裡一樣有個調節參數 β，用來控制偏好強度：

- β 大 → 模型會更嚴格遵守偏好，但語言多樣性可能下降。

- β 小 → 模型比較自由，但偏好效果變弱。

## 🔹為什麼 DPO 更簡單：

- 不需要再訓練 Reward Model。

- 不需要 PPO 的強化學習流程。

- 整體訓練只像在跑一次普通的監督微調（Supervised Fine-Tuning）。

---

# 🔍 三者的技術關鍵差異
| 項目                | RLHF        | PPO          | DPO             |
| ----------------- | ----------- | ------------ | --------------- |
| 核心目的              | 讓模型學人類偏好    | 控制 RLHF 訓練穩定 | 直接根據偏好微調模型      |
| 是否需要 Reward Model | ✅ 需要        | ✅（搭配 RLHF）   | ❌ 不需要           |
| 是否使用強化學習          | ✅ 是         | ✅ 是          | ❌ 否             |
| 是否使用凍結參考模型        | ✅（用於 KL 懲罰） | ✅            | ✅（作為比較基準）       |
| 是否用 KL 散度控制偏移     | ✅ 明確計算      | ✅            | ✅ 隱式透過參考模型      |
| 成本                | 高（昂貴）       | 高            | 低（非常省）          |
| 效果                | 最可控、最靈活     | 穩定更新         | 快速、簡潔、效果接近 RLHF |

---

# 實務該怎麼選？
| 場景                             | 建議                                     |
| ------------------------------ | -------------------------------------- |
| **資源有限、想快速上偏好**                | **DPO**：最省工程 + 成本；多數對話任務已有很好效果         |
| **需要細粒度的獎勵形狀或多維目標（含安全/事實/結構）** | **RLHF (PPO)**：可用多信號 reward 設計，但工程/成本高 |
| **已有高品質偏好配對，但不想訓 RM**          | **DPO**                                |
| **已經有成熟 RL 基礎設施**              | **RLHF (PPO)** 可達更強的上限與可控性             |
| **想要最穩健的 baseline**            | **SFT → DPO**（多數團隊的新標準流程）              |

