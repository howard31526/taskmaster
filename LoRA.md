# LoRA 調整 Llama 3 8B 模型

## 🧠 先講整體：Llama 3 8B 是怎麼構成的

大型語言模型（像 Llama 3 7B 或 8B）是由很多「層」堆起來的，每層都包含兩大模組：
注意力層（Attention）：讓模型可以「看」到輸入中不同位置的關聯。
前饋層（MLP, Multi-Layer Perceptron）：把注意力後的特徵再轉換成更高層的抽象。

## 🧩 名詞解釋
| 名詞             | 中文說法                           | 說明                                                               |
| -------------- | ------------------------------ | ---------------------------------------------------------------- |
| **L**          | 層數 (Layers)                    | 模型有多少層堆疊。例如 Llama 3 8B 大約有 **32 層**。                             |
| **d_model**    | 模型維度                           | 每個詞在模型裡被表示成一個長度為 **4096 維** 的向量。可以想像是「語意座標的維度數」。                 |
| **d_ff**       | 前饋層維度 (Feed-Forward Dimension) | MLP 裡的隱藏層大小，通常是 `d_model` 的 3~4 倍。例如約 **11008 ~ 14336**。         |
| **q, k, v, o** | 注意力裡的 4 個矩陣                    | 分別代表「Query」「Key」「Value」「Output」投影矩陣。每個都是 `d_model × d_model` 大小。 |
| **r**          | LoRA 的秩 (rank)                 | LoRA 的低秩近似大小，常用 8 或 16。越大代表能學到更多變化，但需要更多顯存。                      |


## 🧩 回顧 LoRA 的核心公式
不要直接訓練原始權重矩陣 

這是 LoRA 的更新公式：

$W' = W + \Delta W = W + BA\frac{\alpha}{r}$

這裡：
- 𝑊：原本凍結的模型權重（不訓練）
- A：大小為 𝑟 × 𝑑_𝑖𝑛 的小矩陣
- B：大小為 𝑑_𝑜𝑢𝑡 × 𝑟 的小矩陣
- r：rank（秩），控制 LoRA 的容量（越大代表學得更多）
- α：縮放係數，用來調整更新的強度

訓練過程中 LoRA 只更新 A 和 B，但不同的 rank 值會讓更新的「能量」變多或變少。
舉例：
- 若 r=4，只有 4 維潛在空間
- 若 r=64，潛在空間大 16 倍，更新量也可能大 16 倍

為了不同 r 下學習穩定，LoRA 用  𝛼/𝑟 當作一個「標準化因子」：

- 1/𝑟  會把更新壓小
- 𝛼 再把更新強度調回你想要的程度

## 🔢 常見設定與直覺意義

一般經驗法則：α=2r 等於把 LoRA 更新「放大兩倍」

這樣做的原因是：
- 若 LoRA 太弱（更新太小），模型學不到東西（欠擬合）。
- 若太強，可能破壞原模型分佈（過擬合）。

經驗上，2 倍左右通常穩定又有效，但可以依任務調整（例如 1.0～4.0）

# 計算參數量
只跟 r、層的維度 有關，跟 α 無關
---

## 🧮 LoRA 在每層加了什麼？

LoRA 不是修改整個模型，而是在這些矩陣旁邊多加兩個小矩陣 A,B：
```
𝑊′ = 𝑊 + 𝐵 × 𝐴
```
這兩個矩陣的大小是：
- A：r × d_in
- B：d_out × r

所以每一個被 LoRA 改的線性層，新增的可訓練參數數量是：
```
r×(d_in + d_out)
```
## 套到 Llama 3 8B 的數字
假設：

- d_model = 4096
- d_ff = 14336
- L = 32
- r = 16

那每一層大致有：
| 模組    | 矩陣                   | 尺寸         | LoRA 參數數量（r=16）                                 |
| ----- | -------------------- | ---------- | ----------------------------------------------- |
| 注意力層  | W_q, W_k, W_v, W_o   | 4096×4096  | 每個是 16×(4096+4096)=131,072，共 4 個 → **524,288**  |
| MLP 層 | W_up, W_gate, W_down | 4096↔14336 | 每個是 16×(4096+14336)=294,912，共 3 個 → **884,736** |

每層總計： 524,288+884,736=1,409,024
總共有 32 層： 1,409,024×32≈45,088,768≈4,500萬

---

## 🧩 一句話總結
- Llama 3 8B 共有約 32 層，每層維度約 4096。
- 每層的注意力和 MLP 都是幾個線性層 (q, k, v, o, up, gate, down)。
- LoRA 只在這些線性層加上小小的 2 個矩陣 (rank = 16)。
- 所以全模型只多出大約 4,000 萬個可訓練參數，佔整體約 0.5%。
- 因此可在單張 14~18GB 顯示卡上完成微調。




